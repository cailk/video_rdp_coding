{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "thousand-counter",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/lkcai/anaconda3/envs/video_perc/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from utils import get_dataloader\n",
    "\n",
    "# Define the neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 128)  # Adjust based on input size\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.reshape(-1, 64 * 16 * 16)  # Flatten the output\n",
    "        y = self.relu(self.fc1(x))\n",
    "        x = self.fc2(y)\n",
    "        return x, y\n",
    "\n",
    "# Hyperparameters\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Initialize the network, loss function, and optimizer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Net().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "arctic-feeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Loading MovingMNIST_4_axis_random_sample_step!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/lkcai/anaconda3/envs/video_perc/lib/python3.6/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "step = 20\n",
    "dataset = 'mmnist_4_axis_random_sample_step'\n",
    "loader, _ = get_dataloader(\n",
    "    data_root='./data/', \n",
    "    dataset=dataset, \n",
    "    step=step, \n",
    "    seq_len=3,\n",
    "    batch_size=16, \n",
    "    num_digits=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "italian-lodge",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [100/1], Loss: 2.2832\n",
      "Epoch [1/10], Step [200/1], Loss: 2.3252\n",
      "Epoch [1/10], Step [300/1], Loss: 2.1201\n",
      "Epoch [1/10], Step [400/1], Loss: 2.0522\n",
      "Epoch [1/10], Step [500/1], Loss: 2.0285\n",
      "Epoch [1/10], Step [600/1], Loss: 1.9362\n",
      "Epoch [1/10], Step [700/1], Loss: 1.9767\n",
      "Epoch [1/10], Step [800/1], Loss: 1.9810\n",
      "Epoch [1/10], Step [900/1], Loss: 1.8913\n",
      "Epoch [1/10], Step [1000/1], Loss: 1.8216\n",
      "Epoch [1/10], Step [1100/1], Loss: 1.5007\n",
      "Epoch [1/10], Step [1200/1], Loss: 1.2145\n",
      "Epoch [1/10], Step [1300/1], Loss: 1.6900\n",
      "Epoch [1/10], Step [1400/1], Loss: 1.9781\n",
      "Epoch [1/10], Step [1500/1], Loss: 1.8097\n",
      "Epoch [1/10], Step [1600/1], Loss: 1.3232\n",
      "Epoch [1/10], Step [1700/1], Loss: 1.2967\n",
      "Epoch [1/10], Step [1800/1], Loss: 1.1322\n",
      "Epoch [1/10], Step [1900/1], Loss: 1.4705\n",
      "Epoch [1/10], Step [2000/1], Loss: 1.9496\n",
      "Epoch [1/10], Step [2100/1], Loss: 1.0334\n",
      "Epoch [1/10], Step [2200/1], Loss: 1.2027\n",
      "Epoch [1/10], Step [2300/1], Loss: 1.4678\n",
      "Epoch [1/10], Step [2400/1], Loss: 1.8177\n",
      "Epoch [1/10], Step [2500/1], Loss: 0.6553\n",
      "Epoch [1/10], Step [2600/1], Loss: 0.7105\n",
      "Epoch [1/10], Step [2700/1], Loss: 0.9584\n",
      "Epoch [1/10], Step [2800/1], Loss: 1.4275\n",
      "Epoch [1/10], Step [2900/1], Loss: 0.9578\n",
      "Epoch [1/10], Step [3000/1], Loss: 1.0406\n",
      "Epoch [1/10], Step [3100/1], Loss: 1.2754\n",
      "Epoch [1/10], Step [3200/1], Loss: 1.2355\n",
      "Epoch [1/10], Step [3300/1], Loss: 0.9224\n",
      "Epoch [1/10], Step [3400/1], Loss: 1.3431\n",
      "Epoch [1/10], Step [3500/1], Loss: 1.1419\n",
      "Epoch [1/10], Step [3600/1], Loss: 0.9892\n",
      "Epoch [1/10], Step [3700/1], Loss: 0.9435\n",
      "Epoch [2/10], Step [100/1], Loss: 1.3347\n",
      "Epoch [2/10], Step [200/1], Loss: 1.0162\n",
      "Epoch [2/10], Step [300/1], Loss: 0.8659\n",
      "Epoch [2/10], Step [400/1], Loss: 1.2174\n",
      "Epoch [2/10], Step [500/1], Loss: 1.4850\n",
      "Epoch [2/10], Step [600/1], Loss: 1.0618\n",
      "Epoch [2/10], Step [700/1], Loss: 0.9006\n",
      "Epoch [2/10], Step [800/1], Loss: 1.0596\n",
      "Epoch [2/10], Step [900/1], Loss: 1.0225\n",
      "Epoch [2/10], Step [1000/1], Loss: 1.2525\n",
      "Epoch [2/10], Step [1100/1], Loss: 0.7538\n",
      "Epoch [2/10], Step [1200/1], Loss: 1.2610\n",
      "Epoch [2/10], Step [1300/1], Loss: 0.9224\n",
      "Epoch [2/10], Step [1400/1], Loss: 1.2288\n",
      "Epoch [2/10], Step [1500/1], Loss: 1.0282\n",
      "Epoch [2/10], Step [1600/1], Loss: 0.6982\n",
      "Epoch [2/10], Step [1700/1], Loss: 0.7887\n",
      "Epoch [2/10], Step [1800/1], Loss: 1.3741\n",
      "Epoch [2/10], Step [1900/1], Loss: 0.6201\n",
      "Epoch [2/10], Step [2000/1], Loss: 1.1897\n",
      "Epoch [2/10], Step [2100/1], Loss: 0.7236\n",
      "Epoch [2/10], Step [2200/1], Loss: 0.8586\n",
      "Epoch [2/10], Step [2300/1], Loss: 1.1682\n",
      "Epoch [2/10], Step [2400/1], Loss: 0.5931\n",
      "Epoch [2/10], Step [2500/1], Loss: 0.8901\n",
      "Epoch [2/10], Step [2600/1], Loss: 1.2196\n",
      "Epoch [2/10], Step [2700/1], Loss: 1.1852\n",
      "Epoch [2/10], Step [2800/1], Loss: 1.0905\n",
      "Epoch [2/10], Step [2900/1], Loss: 0.6077\n",
      "Epoch [2/10], Step [3000/1], Loss: 0.9288\n",
      "Epoch [2/10], Step [3100/1], Loss: 0.7964\n",
      "Epoch [2/10], Step [3200/1], Loss: 0.9369\n",
      "Epoch [2/10], Step [3300/1], Loss: 1.0098\n",
      "Epoch [2/10], Step [3400/1], Loss: 0.4037\n",
      "Epoch [2/10], Step [3500/1], Loss: 0.7996\n",
      "Epoch [2/10], Step [3600/1], Loss: 1.2378\n",
      "Epoch [2/10], Step [3700/1], Loss: 0.7396\n",
      "Epoch [3/10], Step [100/1], Loss: 0.5586\n",
      "Epoch [3/10], Step [200/1], Loss: 1.2755\n",
      "Epoch [3/10], Step [300/1], Loss: 1.0562\n",
      "Epoch [3/10], Step [400/1], Loss: 0.9483\n",
      "Epoch [3/10], Step [500/1], Loss: 0.9048\n",
      "Epoch [3/10], Step [600/1], Loss: 1.2046\n",
      "Epoch [3/10], Step [700/1], Loss: 0.9212\n",
      "Epoch [3/10], Step [800/1], Loss: 0.8661\n",
      "Epoch [3/10], Step [900/1], Loss: 0.9796\n",
      "Epoch [3/10], Step [1000/1], Loss: 0.7940\n",
      "Epoch [3/10], Step [1100/1], Loss: 0.7453\n",
      "Epoch [3/10], Step [1200/1], Loss: 0.7901\n",
      "Epoch [3/10], Step [1300/1], Loss: 0.8764\n",
      "Epoch [3/10], Step [1400/1], Loss: 0.5082\n",
      "Epoch [3/10], Step [1500/1], Loss: 0.9562\n",
      "Epoch [3/10], Step [1600/1], Loss: 1.2337\n",
      "Epoch [3/10], Step [1700/1], Loss: 1.1131\n",
      "Epoch [3/10], Step [1800/1], Loss: 0.4398\n",
      "Epoch [3/10], Step [1900/1], Loss: 0.8024\n",
      "Epoch [3/10], Step [2000/1], Loss: 1.0749\n",
      "Epoch [3/10], Step [2100/1], Loss: 0.5360\n",
      "Epoch [3/10], Step [2200/1], Loss: 0.8170\n",
      "Epoch [3/10], Step [2300/1], Loss: 0.7834\n",
      "Epoch [3/10], Step [2400/1], Loss: 0.6304\n",
      "Epoch [3/10], Step [2500/1], Loss: 0.4596\n",
      "Epoch [3/10], Step [2600/1], Loss: 1.0693\n",
      "Epoch [3/10], Step [2700/1], Loss: 0.3915\n",
      "Epoch [3/10], Step [2800/1], Loss: 0.9241\n",
      "Epoch [3/10], Step [2900/1], Loss: 0.9559\n",
      "Epoch [3/10], Step [3000/1], Loss: 0.4734\n",
      "Epoch [3/10], Step [3100/1], Loss: 0.9133\n",
      "Epoch [3/10], Step [3200/1], Loss: 1.0084\n",
      "Epoch [3/10], Step [3300/1], Loss: 0.9458\n",
      "Epoch [3/10], Step [3400/1], Loss: 0.6520\n",
      "Epoch [3/10], Step [3500/1], Loss: 0.9575\n",
      "Epoch [3/10], Step [3600/1], Loss: 1.1123\n",
      "Epoch [3/10], Step [3700/1], Loss: 1.0773\n",
      "Epoch [4/10], Step [100/1], Loss: 0.7890\n",
      "Epoch [4/10], Step [200/1], Loss: 0.7668\n",
      "Epoch [4/10], Step [300/1], Loss: 0.9821\n",
      "Epoch [4/10], Step [400/1], Loss: 0.8434\n",
      "Epoch [4/10], Step [500/1], Loss: 0.8665\n",
      "Epoch [4/10], Step [600/1], Loss: 0.9394\n",
      "Epoch [4/10], Step [700/1], Loss: 0.8795\n",
      "Epoch [4/10], Step [800/1], Loss: 1.9262\n",
      "Epoch [4/10], Step [900/1], Loss: 0.9212\n",
      "Epoch [4/10], Step [1000/1], Loss: 0.5883\n",
      "Epoch [4/10], Step [1100/1], Loss: 1.1364\n",
      "Epoch [4/10], Step [1200/1], Loss: 0.8531\n",
      "Epoch [4/10], Step [1300/1], Loss: 0.4970\n",
      "Epoch [4/10], Step [1400/1], Loss: 1.2113\n",
      "Epoch [4/10], Step [1500/1], Loss: 0.7513\n",
      "Epoch [4/10], Step [1600/1], Loss: 0.8143\n",
      "Epoch [4/10], Step [1700/1], Loss: 0.5843\n",
      "Epoch [4/10], Step [1800/1], Loss: 1.0396\n",
      "Epoch [4/10], Step [1900/1], Loss: 0.8797\n",
      "Epoch [4/10], Step [2000/1], Loss: 0.7024\n",
      "Epoch [4/10], Step [2100/1], Loss: 0.4871\n",
      "Epoch [4/10], Step [2200/1], Loss: 0.7841\n",
      "Epoch [4/10], Step [2300/1], Loss: 0.4220\n",
      "Epoch [4/10], Step [2400/1], Loss: 0.7641\n",
      "Epoch [4/10], Step [2500/1], Loss: 0.8040\n",
      "Epoch [4/10], Step [2600/1], Loss: 0.8680\n",
      "Epoch [4/10], Step [2700/1], Loss: 0.6556\n",
      "Epoch [4/10], Step [2800/1], Loss: 0.5581\n",
      "Epoch [4/10], Step [2900/1], Loss: 1.6161\n",
      "Epoch [4/10], Step [3000/1], Loss: 0.8848\n",
      "Epoch [4/10], Step [3100/1], Loss: 1.0429\n",
      "Epoch [4/10], Step [3200/1], Loss: 1.0537\n",
      "Epoch [4/10], Step [3300/1], Loss: 0.9820\n",
      "Epoch [4/10], Step [3400/1], Loss: 0.7455\n",
      "Epoch [4/10], Step [3500/1], Loss: 0.5023\n",
      "Epoch [4/10], Step [3600/1], Loss: 1.3058\n",
      "Epoch [4/10], Step [3700/1], Loss: 0.5021\n",
      "Epoch [5/10], Step [100/1], Loss: 1.5961\n",
      "Epoch [5/10], Step [200/1], Loss: 0.6349\n",
      "Epoch [5/10], Step [300/1], Loss: 0.3431\n",
      "Epoch [5/10], Step [400/1], Loss: 1.1731\n",
      "Epoch [5/10], Step [500/1], Loss: 1.1965\n",
      "Epoch [5/10], Step [600/1], Loss: 0.6803\n",
      "Epoch [5/10], Step [700/1], Loss: 0.7621\n",
      "Epoch [5/10], Step [800/1], Loss: 0.8107\n",
      "Epoch [5/10], Step [900/1], Loss: 0.4155\n",
      "Epoch [5/10], Step [1000/1], Loss: 1.0403\n",
      "Epoch [5/10], Step [1100/1], Loss: 0.7030\n",
      "Epoch [5/10], Step [1200/1], Loss: 0.9591\n",
      "Epoch [5/10], Step [1300/1], Loss: 1.1994\n",
      "Epoch [5/10], Step [1400/1], Loss: 0.5719\n",
      "Epoch [5/10], Step [1500/1], Loss: 0.8214\n",
      "Epoch [5/10], Step [1600/1], Loss: 0.5021\n",
      "Epoch [5/10], Step [1700/1], Loss: 0.9743\n",
      "Epoch [5/10], Step [1800/1], Loss: 0.4426\n",
      "Epoch [5/10], Step [1900/1], Loss: 0.7352\n",
      "Epoch [5/10], Step [2000/1], Loss: 0.9816\n",
      "Epoch [5/10], Step [2100/1], Loss: 1.1821\n",
      "Epoch [5/10], Step [2200/1], Loss: 0.7699\n",
      "Epoch [5/10], Step [2300/1], Loss: 1.1262\n",
      "Epoch [5/10], Step [2400/1], Loss: 0.9387\n",
      "Epoch [5/10], Step [2500/1], Loss: 0.6332\n",
      "Epoch [5/10], Step [2600/1], Loss: 1.0911\n",
      "Epoch [5/10], Step [2700/1], Loss: 0.5644\n",
      "Epoch [5/10], Step [2800/1], Loss: 0.4995\n",
      "Epoch [5/10], Step [2900/1], Loss: 0.7834\n",
      "Epoch [5/10], Step [3000/1], Loss: 0.8422\n",
      "Epoch [5/10], Step [3100/1], Loss: 1.0286\n",
      "Epoch [5/10], Step [3200/1], Loss: 0.5712\n",
      "Epoch [5/10], Step [3300/1], Loss: 0.9803\n",
      "Epoch [5/10], Step [3400/1], Loss: 0.9053\n",
      "Epoch [5/10], Step [3500/1], Loss: 0.6748\n",
      "Epoch [5/10], Step [3600/1], Loss: 0.6876\n",
      "Epoch [5/10], Step [3700/1], Loss: 0.6748\n",
      "Epoch [6/10], Step [100/1], Loss: 0.6194\n",
      "Epoch [6/10], Step [200/1], Loss: 0.7213\n",
      "Epoch [6/10], Step [300/1], Loss: 0.6686\n",
      "Epoch [6/10], Step [400/1], Loss: 0.7227\n",
      "Epoch [6/10], Step [500/1], Loss: 0.8481\n",
      "Epoch [6/10], Step [600/1], Loss: 0.1883\n",
      "Epoch [6/10], Step [700/1], Loss: 0.6287\n",
      "Epoch [6/10], Step [800/1], Loss: 0.9578\n",
      "Epoch [6/10], Step [900/1], Loss: 0.8075\n",
      "Epoch [6/10], Step [1000/1], Loss: 0.6594\n",
      "Epoch [6/10], Step [1100/1], Loss: 0.7270\n",
      "Epoch [6/10], Step [1200/1], Loss: 1.0484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/10], Step [1300/1], Loss: 0.9050\n",
      "Epoch [6/10], Step [1400/1], Loss: 0.8704\n",
      "Epoch [6/10], Step [1500/1], Loss: 0.3890\n",
      "Epoch [6/10], Step [1600/1], Loss: 0.4950\n",
      "Epoch [6/10], Step [1700/1], Loss: 0.9727\n",
      "Epoch [6/10], Step [1800/1], Loss: 0.7707\n",
      "Epoch [6/10], Step [1900/1], Loss: 0.7810\n",
      "Epoch [6/10], Step [2000/1], Loss: 0.9569\n",
      "Epoch [6/10], Step [2100/1], Loss: 0.4645\n",
      "Epoch [6/10], Step [2200/1], Loss: 1.1462\n",
      "Epoch [6/10], Step [2300/1], Loss: 1.1315\n",
      "Epoch [6/10], Step [2400/1], Loss: 0.9998\n",
      "Epoch [6/10], Step [2500/1], Loss: 0.5618\n",
      "Epoch [6/10], Step [2600/1], Loss: 0.9239\n",
      "Epoch [6/10], Step [2700/1], Loss: 0.8518\n",
      "Epoch [6/10], Step [2800/1], Loss: 0.7255\n",
      "Epoch [6/10], Step [2900/1], Loss: 0.8271\n",
      "Epoch [6/10], Step [3000/1], Loss: 1.2214\n",
      "Epoch [6/10], Step [3100/1], Loss: 0.8520\n",
      "Epoch [6/10], Step [3200/1], Loss: 0.7138\n",
      "Epoch [6/10], Step [3300/1], Loss: 1.1726\n",
      "Epoch [6/10], Step [3400/1], Loss: 0.8609\n",
      "Epoch [6/10], Step [3500/1], Loss: 0.7041\n",
      "Epoch [6/10], Step [3600/1], Loss: 1.0123\n",
      "Epoch [6/10], Step [3700/1], Loss: 1.2065\n",
      "Epoch [7/10], Step [100/1], Loss: 0.8635\n",
      "Epoch [7/10], Step [200/1], Loss: 0.9853\n",
      "Epoch [7/10], Step [300/1], Loss: 1.0671\n",
      "Epoch [7/10], Step [400/1], Loss: 0.4988\n",
      "Epoch [7/10], Step [500/1], Loss: 0.4763\n",
      "Epoch [7/10], Step [600/1], Loss: 0.2932\n",
      "Epoch [7/10], Step [700/1], Loss: 0.6545\n",
      "Epoch [7/10], Step [800/1], Loss: 0.3137\n",
      "Epoch [7/10], Step [900/1], Loss: 0.4893\n",
      "Epoch [7/10], Step [1000/1], Loss: 0.3554\n",
      "Epoch [7/10], Step [1100/1], Loss: 1.0184\n",
      "Epoch [7/10], Step [1200/1], Loss: 0.8331\n",
      "Epoch [7/10], Step [1300/1], Loss: 0.8455\n",
      "Epoch [7/10], Step [1400/1], Loss: 0.3124\n",
      "Epoch [7/10], Step [1500/1], Loss: 0.5316\n",
      "Epoch [7/10], Step [1600/1], Loss: 0.8851\n",
      "Epoch [7/10], Step [1700/1], Loss: 1.0404\n",
      "Epoch [7/10], Step [1800/1], Loss: 0.5908\n",
      "Epoch [7/10], Step [1900/1], Loss: 0.8622\n",
      "Epoch [7/10], Step [2000/1], Loss: 0.6866\n",
      "Epoch [7/10], Step [2100/1], Loss: 0.9070\n",
      "Epoch [7/10], Step [2200/1], Loss: 0.3976\n",
      "Epoch [7/10], Step [2300/1], Loss: 0.7580\n",
      "Epoch [7/10], Step [2400/1], Loss: 0.9897\n",
      "Epoch [7/10], Step [2500/1], Loss: 0.6864\n",
      "Epoch [7/10], Step [2600/1], Loss: 0.7275\n",
      "Epoch [7/10], Step [2700/1], Loss: 0.9731\n",
      "Epoch [7/10], Step [2800/1], Loss: 0.9606\n",
      "Epoch [7/10], Step [2900/1], Loss: 1.2206\n",
      "Epoch [7/10], Step [3000/1], Loss: 0.8154\n",
      "Epoch [7/10], Step [3100/1], Loss: 0.7007\n",
      "Epoch [7/10], Step [3200/1], Loss: 1.0359\n",
      "Epoch [7/10], Step [3300/1], Loss: 0.9990\n",
      "Epoch [7/10], Step [3400/1], Loss: 0.4863\n",
      "Epoch [7/10], Step [3500/1], Loss: 0.9718\n",
      "Epoch [7/10], Step [3600/1], Loss: 0.7185\n",
      "Epoch [7/10], Step [3700/1], Loss: 0.6339\n",
      "Epoch [8/10], Step [100/1], Loss: 1.1630\n",
      "Epoch [8/10], Step [200/1], Loss: 0.2567\n",
      "Epoch [8/10], Step [300/1], Loss: 1.2795\n",
      "Epoch [8/10], Step [400/1], Loss: 0.7815\n",
      "Epoch [8/10], Step [500/1], Loss: 0.7139\n",
      "Epoch [8/10], Step [600/1], Loss: 0.8521\n",
      "Epoch [8/10], Step [700/1], Loss: 0.8946\n",
      "Epoch [8/10], Step [800/1], Loss: 1.2778\n",
      "Epoch [8/10], Step [900/1], Loss: 0.5473\n",
      "Epoch [8/10], Step [1000/1], Loss: 0.5986\n",
      "Epoch [8/10], Step [1100/1], Loss: 0.5789\n",
      "Epoch [8/10], Step [1200/1], Loss: 0.6860\n",
      "Epoch [8/10], Step [1300/1], Loss: 1.0617\n",
      "Epoch [8/10], Step [1400/1], Loss: 1.0162\n",
      "Epoch [8/10], Step [1500/1], Loss: 0.6307\n",
      "Epoch [8/10], Step [1600/1], Loss: 0.3944\n",
      "Epoch [8/10], Step [1700/1], Loss: 0.7611\n",
      "Epoch [8/10], Step [1800/1], Loss: 0.6900\n",
      "Epoch [8/10], Step [1900/1], Loss: 1.2452\n",
      "Epoch [8/10], Step [2000/1], Loss: 1.2404\n",
      "Epoch [8/10], Step [2100/1], Loss: 0.7679\n",
      "Epoch [8/10], Step [2200/1], Loss: 0.3873\n",
      "Epoch [8/10], Step [2300/1], Loss: 0.7453\n",
      "Epoch [8/10], Step [2400/1], Loss: 1.0466\n",
      "Epoch [8/10], Step [2500/1], Loss: 0.3401\n",
      "Epoch [8/10], Step [2600/1], Loss: 0.6086\n",
      "Epoch [8/10], Step [2700/1], Loss: 0.7316\n",
      "Epoch [8/10], Step [2800/1], Loss: 1.2237\n",
      "Epoch [8/10], Step [2900/1], Loss: 0.7185\n",
      "Epoch [8/10], Step [3000/1], Loss: 0.5219\n",
      "Epoch [8/10], Step [3100/1], Loss: 0.8391\n",
      "Epoch [8/10], Step [3200/1], Loss: 0.5411\n",
      "Epoch [8/10], Step [3300/1], Loss: 0.5047\n",
      "Epoch [8/10], Step [3400/1], Loss: 0.5113\n",
      "Epoch [8/10], Step [3500/1], Loss: 1.0733\n",
      "Epoch [8/10], Step [3600/1], Loss: 0.3312\n",
      "Epoch [8/10], Step [3700/1], Loss: 1.3658\n",
      "Epoch [9/10], Step [100/1], Loss: 0.6999\n",
      "Epoch [9/10], Step [200/1], Loss: 0.9722\n",
      "Epoch [9/10], Step [300/1], Loss: 0.7240\n",
      "Epoch [9/10], Step [400/1], Loss: 0.5651\n",
      "Epoch [9/10], Step [500/1], Loss: 0.5490\n",
      "Epoch [9/10], Step [600/1], Loss: 0.7331\n",
      "Epoch [9/10], Step [700/1], Loss: 0.9879\n",
      "Epoch [9/10], Step [800/1], Loss: 1.1607\n",
      "Epoch [9/10], Step [900/1], Loss: 0.4853\n",
      "Epoch [9/10], Step [1000/1], Loss: 0.4545\n",
      "Epoch [9/10], Step [1100/1], Loss: 0.8105\n",
      "Epoch [9/10], Step [1200/1], Loss: 0.3588\n",
      "Epoch [9/10], Step [1300/1], Loss: 0.9316\n",
      "Epoch [9/10], Step [1400/1], Loss: 0.6325\n",
      "Epoch [9/10], Step [1500/1], Loss: 0.5991\n",
      "Epoch [9/10], Step [1600/1], Loss: 0.7406\n",
      "Epoch [9/10], Step [1700/1], Loss: 0.5134\n",
      "Epoch [9/10], Step [1800/1], Loss: 0.3946\n",
      "Epoch [9/10], Step [1900/1], Loss: 0.4693\n",
      "Epoch [9/10], Step [2000/1], Loss: 0.2601\n",
      "Epoch [9/10], Step [2100/1], Loss: 0.7178\n",
      "Epoch [9/10], Step [2200/1], Loss: 0.8896\n",
      "Epoch [9/10], Step [2300/1], Loss: 0.6208\n",
      "Epoch [9/10], Step [2400/1], Loss: 0.8985\n",
      "Epoch [9/10], Step [2500/1], Loss: 0.6042\n",
      "Epoch [9/10], Step [2600/1], Loss: 1.3598\n",
      "Epoch [9/10], Step [2700/1], Loss: 0.9789\n",
      "Epoch [9/10], Step [2800/1], Loss: 1.0870\n",
      "Epoch [9/10], Step [2900/1], Loss: 0.4208\n",
      "Epoch [9/10], Step [3000/1], Loss: 1.0042\n",
      "Epoch [9/10], Step [3100/1], Loss: 0.3253\n",
      "Epoch [9/10], Step [3200/1], Loss: 0.2953\n",
      "Epoch [9/10], Step [3300/1], Loss: 0.8064\n",
      "Epoch [9/10], Step [3400/1], Loss: 0.9443\n",
      "Epoch [9/10], Step [3500/1], Loss: 0.7046\n",
      "Epoch [9/10], Step [3600/1], Loss: 0.3946\n",
      "Epoch [9/10], Step [3700/1], Loss: 1.0673\n",
      "Epoch [10/10], Step [100/1], Loss: 0.5564\n",
      "Epoch [10/10], Step [200/1], Loss: 0.3564\n",
      "Epoch [10/10], Step [300/1], Loss: 0.5080\n",
      "Epoch [10/10], Step [400/1], Loss: 0.9349\n",
      "Epoch [10/10], Step [500/1], Loss: 0.4203\n",
      "Epoch [10/10], Step [600/1], Loss: 0.4748\n",
      "Epoch [10/10], Step [700/1], Loss: 0.9173\n",
      "Epoch [10/10], Step [800/1], Loss: 0.2781\n",
      "Epoch [10/10], Step [900/1], Loss: 1.2037\n",
      "Epoch [10/10], Step [1000/1], Loss: 0.6260\n",
      "Epoch [10/10], Step [1100/1], Loss: 0.5894\n",
      "Epoch [10/10], Step [1200/1], Loss: 0.7332\n",
      "Epoch [10/10], Step [1300/1], Loss: 0.5767\n",
      "Epoch [10/10], Step [1400/1], Loss: 1.2817\n",
      "Epoch [10/10], Step [1500/1], Loss: 0.4226\n",
      "Epoch [10/10], Step [1600/1], Loss: 0.3978\n",
      "Epoch [10/10], Step [1700/1], Loss: 0.7483\n",
      "Epoch [10/10], Step [1800/1], Loss: 0.4263\n",
      "Epoch [10/10], Step [1900/1], Loss: 0.7224\n",
      "Epoch [10/10], Step [2000/1], Loss: 0.8896\n",
      "Epoch [10/10], Step [2100/1], Loss: 0.9615\n",
      "Epoch [10/10], Step [2200/1], Loss: 0.1410\n",
      "Epoch [10/10], Step [2300/1], Loss: 1.5179\n",
      "Epoch [10/10], Step [2400/1], Loss: 0.5023\n",
      "Epoch [10/10], Step [2500/1], Loss: 0.9310\n",
      "Epoch [10/10], Step [2600/1], Loss: 0.5591\n",
      "Epoch [10/10], Step [2700/1], Loss: 1.0292\n",
      "Epoch [10/10], Step [2800/1], Loss: 0.9751\n",
      "Epoch [10/10], Step [2900/1], Loss: 0.5885\n",
      "Epoch [10/10], Step [3000/1], Loss: 0.7080\n",
      "Epoch [10/10], Step [3100/1], Loss: 0.5197\n",
      "Epoch [10/10], Step [3200/1], Loss: 0.4470\n",
      "Epoch [10/10], Step [3300/1], Loss: 0.4599\n",
      "Epoch [10/10], Step [3400/1], Loss: 0.7249\n",
      "Epoch [10/10], Step [3500/1], Loss: 0.2937\n",
      "Epoch [10/10], Step [3600/1], Loss: 0.6131\n",
      "Epoch [10/10], Step [3700/1], Loss: 0.5741\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for i,x in enumerate(iter(loader)):\n",
    "        #Get the data\n",
    "        image = x[0]\n",
    "        label = x[1].cuda()\n",
    "        image = image.permute(0, 4, 1, 2, 3)\n",
    "        image = image.cuda().float()\n",
    "\n",
    "        \n",
    "        label_ = label\n",
    "        # Forward pass\n",
    "        outputs,_ = model(image[:, :, 2, ...])\n",
    "        loss = criterion(outputs, label_)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                  .format(epoch+1, num_epochs, i+1, 1, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "usual-denver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4, device='cuda:0')\n",
      "tensor(4, device='cuda:0')\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "k = 14\n",
    "j=1\n",
    "plt.imshow(image[k,0,j,...].detach().cpu().numpy())\n",
    "print (label[k])\n",
    "print (outputs.argmax(dim=1)[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "protected-firewall",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/lkcai/anaconda3/envs/video_perc/lib/python3.6/site-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7281666666666666\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i,x in enumerate(iter(loader)):\n",
    "        #Get the data\n",
    "        image = x[0]\n",
    "        label = x[1].cuda()\n",
    "        image = image.permute(0, 4, 1, 2, 3)\n",
    "        image = image.cuda().float()\n",
    "\n",
    "\n",
    "        label_ = label\n",
    "        outputs,_ = model(image[:,:,2,...])\n",
    "        correct += (outputs.argmax(dim=1) == label_).sum().item()\n",
    "        total+= label.size(0)\n",
    "\n",
    "print (correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "second-copyright",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (3.6)",
   "language": "python",
   "name": "video_perc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
