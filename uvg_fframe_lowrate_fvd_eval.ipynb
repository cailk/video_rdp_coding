{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "capital-diesel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 26 15:43:58 2025       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                               |                      |               MIG M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  Tesla T4            On   | 00000000:86:00.0 Off |                    0 |\r\n",
      "| N/A   42C    P0    28W /  70W |  14519MiB / 15360MiB |      0%      Default |\r\n",
      "|                               |                      |                  N/A |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                  |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
      "|        ID   ID                                                   Usage      |\r\n",
      "|=============================================================================|\r\n",
      "|    0   N/A  N/A     12214      C   ...nvs/video_perc/bin/python     1136MiB |\r\n",
      "|    0   N/A  N/A     20885      C   ...nvs/video_perc/bin/python     3610MiB |\r\n",
      "|    0   N/A  N/A     23620      C   ...nvs/video_perc/bin/python     9770MiB |\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import ffmpeg\n",
    "import itertools\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms.functional import resize\n",
    "from pytorch_msssim import ms_ssim, ssim\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import lpips\n",
    "\n",
    "dcvc_path = os.path.abspath(\"/h/lkcai/code/video-perception/DCVC_HEM\")\n",
    "if dcvc_path not in sys.path:\n",
    "    sys.path.insert(0, dcvc_path)\n",
    "    \n",
    "hific_path = os.path.abspath(\"/h/lkcai/code/video-perception/hific\")\n",
    "if hific_path not in sys.path:\n",
    "    sys.path.insert(0, hific_path)\n",
    "\n",
    "thirdparty_repo_path = os.path.abspath(\"/h/lkcai/code/video-perception/video_quality_metrics\")\n",
    "if thirdparty_repo_path not in sys.path:\n",
    "    sys.path.insert(0, thirdparty_repo_path)\n",
    "\n",
    "from hific.compress import prepare_model, prepare_dataloader, \\\n",
    "    compress_and_save, load_and_decompress, compress_and_decompress\n",
    "\n",
    "from UVG1 import UVG\n",
    "from ssf_model import ScaleSpaceFlow\n",
    "from DCVC_HEM.src.models.video_model import DMC\n",
    "from DCVC_HEM.src.utils.stream_helper import get_padding_size, get_state_dict\n",
    "\n",
    "from video_quality_metrics.calculate_fvd import calculate_fvd\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available else cpu)\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "authentic-halifax",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ssf_model(model, pre_path):\n",
    "    model.motion_encoder.load_state_dict(torch.load(pre_path + '/m_enc.pth'))\n",
    "    model.motion_decoder.load_state_dict(torch.load(pre_path + '/m_dec.pth'))\n",
    "    model.P_encoder.load_state_dict(torch.load(pre_path + '/p_enc.pth'))\n",
    "    model.res_encoder.load_state_dict(torch.load(pre_path + '/r_enc.pth'))\n",
    "    model.res_decoder.load_state_dict(torch.load(pre_path + '/r_dec.pth'))\n",
    "    return model\n",
    "\n",
    "def hwc_tonp(x):\n",
    "    x = x.detach().cpu().numpy()\n",
    "    x = x.transpose([0, 2, 3, 1])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "married-morocco",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: /h/lkcai/anaconda3/envs/video_perc/lib/python3.6/site-packages/lpips/weights/v0.1/alex.pth\n"
     ]
    }
   ],
   "source": [
    "def PSNR(input1, input2):\n",
    "    mse = torch.mean((input1 - input2) ** 2)\n",
    "    psnr = 20 * torch.log10(1 / torch.sqrt(mse))\n",
    "    return psnr.item()\n",
    "\n",
    "def MS_SSIM(v1, v2):\n",
    "    # [0, 1]\n",
    "    v1 = (v1 + 1) * 0.5\n",
    "    v2 = (v2 + 1) * 0.5\n",
    "    return ssim(v1, v2, data_range=1, size_average=True).item()\n",
    "\n",
    "lpips_vgg = lpips.LPIPS(net='alex').to(device)\n",
    "def LPIPS(img1, img2):\n",
    "    # img1 = img1 * 2 - 1\n",
    "    # img2 = img2 * 2 - 1\n",
    "    img1 = (img1 + 1) * 0.5\n",
    "    img2 = (img2 + 1) * 0.5\n",
    "    with torch.no_grad():\n",
    "        lpips = lpips_vgg(img1, img2)\n",
    "\n",
    "    return lpips.mean().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "classical-muscle",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = './DCVC_HEM/checkpoints/acmmm2022_video_ssim.pth.tar'\n",
    "p_frame_y_q_scales, p_frame_mv_y_q_scales = DMC.get_q_scales_from_ckpt(model_path)\n",
    "\n",
    "p_state_dict = get_state_dict(model_path)\n",
    "video_net = DMC()\n",
    "video_net.load_state_dict(p_state_dict)\n",
    "video_net = video_net.to(device)\n",
    "video_net.eval()\n",
    "\n",
    "def run_dcvc_test(x, rate_idx=0):\n",
    "    x2     = x[:, 1, ...]\n",
    "    x3     = x[:, 2, ...]\n",
    "    x1_hat = x[:, 3, ...]\n",
    "    \n",
    "    p_frame_y_q_scale = p_frame_y_q_scales[rate_idx]\n",
    "    p_frame_mv_y_q_scale = p_frame_mv_y_q_scales[rate_idx]\n",
    "\n",
    "    bbp_2 = []\n",
    "    bbp_3 = []\n",
    "    with torch.no_grad():\n",
    "        h, w = x2.shape[2], x2.shape[3]\n",
    "        bin_path = None\n",
    "        dpb = {\n",
    "            \"ref_frame\": x1_hat, \"ref_feature\": None, \"ref_y\": None, \"ref_mv_y\": None,\n",
    "        }\n",
    "        x2_result = video_net.encode_decode(\n",
    "            x2, dpb, bin_path,\n",
    "            pic_height=h, pic_width=w,\n",
    "            mv_y_q_scale=p_frame_mv_y_q_scale,\n",
    "            y_q_scale=p_frame_y_q_scale\n",
    "        )\n",
    "        dpb = x2_result[\"dpb\"]\n",
    "        x2_hat = dpb[\"ref_frame\"].clamp_(0, 1)\n",
    "        bbp_2.append(x2_result['bit'])\n",
    "\n",
    "        x3_result = video_net.encode_decode(\n",
    "            x3, dpb, bin_path,\n",
    "            pic_height=h, pic_width=w,\n",
    "            mv_y_q_scale=p_frame_mv_y_q_scale,\n",
    "            y_q_scale=p_frame_y_q_scale\n",
    "        )\n",
    "        dpb = x3_result[\"dpb\"]\n",
    "        x3_hat = dpb[\"ref_frame\"].clamp_(0, 1)\n",
    "        bbp_3.append(x3_result['bit'])\n",
    "\n",
    "    avg_bbp_2 = sum(bbp_2) / len(bbp_2)\n",
    "    avg_bbp_3 = sum(bbp_3) / len(bbp_3)\n",
    "\n",
    "    return x2_hat, x3_hat, avg_bbp_2, avg_bbp_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "solid-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "    transforms.ToTensor(), \n",
    "    transforms.RandomCrop(256)\n",
    "])\n",
    "\n",
    "uvg_dataset = UVG(\"./data/uvg/\", train_transforms)\n",
    "uvg_dataloader = DataLoader(\n",
    "    uvg_dataset,\n",
    "    batch_size=5,\n",
    "    num_workers=0,\n",
    "    shuffle=True,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "wired-manor",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_AR = 0.08\n",
    "\n",
    "ssf_JD = ScaleSpaceFlow().to(device)\n",
    "ssf_JD.load_state_dict(torch.load('./saved_models/vimeo-90k/JD/ssf_uvg_JD.pth'))\n",
    "\n",
    "ssf_AR = ScaleSpaceFlow().to(device)\n",
    "ssf_AR = load_ssf_model(ssf_AR, f'./saved_models/vimeo-90k/AR_{l_AR}/')\n",
    "\n",
    "ssf_MSE = ScaleSpaceFlow().to(device)\n",
    "ssf_MSE = load_ssf_model(ssf_MSE, f'./saved_models/vimeo-90k/mse/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "treated-quebec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:01:11 INFO - logger_setup: /fs01/home/lkcai/code/video-perception/hific/compress.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building prior probability tables...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 250.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up Perceptual loss...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:01:17 INFO - load_model: Loading model ...\n",
      "15:01:17 INFO - load_model: Estimated model size (under fp32): 725.903 MB\n",
      "15:01:17 INFO - load_model: Model init 5.602s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: /h/lkcai/code/video-perception/hific/src/loss/perceptual_similarity/weights/v0.1/alex.pth\n",
      "...[net-lin [alex]] initialized\n",
      "...Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15:01:17 INFO - prepare_model: Model loaded from disk.\n",
      "15:01:17 INFO - prepare_model: Building hyperprior probability tables...\n",
      "100%|██████████| 320/320 [00:00<00:00, 1215.16it/s]\n",
      "15:01:28 INFO - prepare_model: All tables built.\n"
     ]
    }
   ],
   "source": [
    "hific_model_path = \"./saved_models/vimeo-90k/FMD/hific_hi.pt\"\n",
    "hific_log_path = \"./hific/log\"\n",
    "\n",
    "fmd, args = prepare_model(hific_model_path, hific_log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "exotic-mambo",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [03:23, 20.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FVD:    MSE 56.4959 | JD 690.8981 | FMD 65.5926 | AR 35.1941 | DCVC 76.0244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Evaluate FVD\n",
    "mse_eval = {'fvd': []} # {'2_psnr':[], '3_psnr': [], '2_ssim': [], '3_ssim': []}\n",
    "jd_eval  = {'fvd': []} # {'2_psnr':[], '3_psnr': [], '2_ssim': [], '3_ssim': []}\n",
    "fmd_eval = {'fvd': []} # {'2_psnr':[], '3_psnr': [], '2_ssim': [], '3_ssim': []}\n",
    "ar_eval  = {'fvd': []} # {'2_psnr':[], '3_psnr': [], '2_ssim': [], '3_ssim': []}\n",
    "dcvc_eval = {'fvd': []} # {'2_psnr':[], '3_psnr': [], '2_ssim': [], '3_ssim': []}\n",
    "\n",
    "dcvc_bbp_x2 = []\n",
    "dcvc_bbp_x3 = []\n",
    "\n",
    "length = 10\n",
    "test_loader = itertools.islice(uvg_dataloader, length)\n",
    "\n",
    "ssf_JD.eval()\n",
    "ssf_AR.eval()\n",
    "ssf_MSE.eval()\n",
    "fmd.eval()\n",
    "\n",
    "for data in tqdm(test_loader):\n",
    "    with torch.no_grad():\n",
    "        x = data[:, :4, ...].to(device)\n",
    "        # x1 = 2 * (data[:, 0, ...] - 0.5).to(device) # origin first frame\n",
    "        x2 = 2 * (data[:, 1, ...] - 0.5).to(device)\n",
    "        x3 = 2 * (data[:, 2, ...] - 0.5).to(device)\n",
    "        x1_hat = 2 * (data[:, 3, ...] - 0.5).to(device) # low-rate first frame\n",
    "        \n",
    "        x_fvd = 2 * (x[:, 1:3, ...] - 0.5)\n",
    "        x_fvd = x_fvd.view(1, -1, 3, 256, 256)\n",
    "\n",
    "        x2_hat_JD = ssf_JD([x1_hat, x2])\n",
    "        x3_hat_JD = ssf_JD([x2_hat_JD, x3])\n",
    "        x_fvd_JD = torch.cat([x2_hat_JD.unsqueeze(1), x3_hat_JD.unsqueeze(1)], dim=1)\n",
    "        x_fvd_JD = x_fvd_JD.view(1, -1, 3, 256, 256)\n",
    "        \n",
    "        x2_hat_AR = ssf_AR([x1_hat, x2])\n",
    "        x3_hat_AR = ssf_AR([x2_hat_AR, x3])\n",
    "        x_fvd_AR = torch.cat([x2_hat_AR.unsqueeze(1), x3_hat_AR.unsqueeze(1)], dim=1)\n",
    "        x_fvd_AR = x_fvd_AR.view(1, -1, 3, 256, 256)\n",
    "        \n",
    "        x2_hat_MSE = ssf_MSE([x1_hat, x2])\n",
    "        x3_hat_MSE = ssf_MSE([x2_hat_MSE, x3])\n",
    "        x_fvd_MSE = torch.cat([x2_hat_MSE.unsqueeze(1), x3_hat_MSE.unsqueeze(1)], dim=1)\n",
    "        x_fvd_MSE = x_fvd_MSE.view(1, -1, 3, 256, 256)    \n",
    "\n",
    "        x2_hat_FMD, _ = fmd(x2)\n",
    "        x3_hat_FMD, _ = fmd(x3)\n",
    "        x_fvd_FMD = torch.cat([x2_hat_FMD.unsqueeze(1), x3_hat_FMD.unsqueeze(1)], dim=1)\n",
    "        x_fvd_FMD = x_fvd_FMD.view(1, -1, 3, 256, 256)\n",
    "\n",
    "        x2_hat_dcvc, x3_hat_dcvc, bbp_x2, bbp_x3 = run_dcvc_test(x, rate_idx=3)\n",
    "        dcvc_bbp_x2.append(bbp_x2)\n",
    "        dcvc_bbp_x3.append(bbp_x3)\n",
    "        x_fvd_dcvc = torch.cat([x2_hat_dcvc.unsqueeze(1), x3_hat_dcvc.unsqueeze(1)], dim=1)\n",
    "        x_fvd_dcvc = x_fvd_dcvc.view(1, -1, 3, 256, 256)\n",
    "\n",
    "        mse_eval['fvd'].append(calculate_fvd(x_fvd, x_fvd_MSE, device, only_final=True)['value'])\n",
    "        jd_eval['fvd'].append(calculate_fvd(x_fvd, x_fvd_JD, device, only_final=True)['value'])\n",
    "        fmd_eval['fvd'].append(calculate_fvd(x_fvd, 2 * x_fvd_FMD - 1., device, only_final=True)['value'])\n",
    "        ar_eval['fvd'].append(calculate_fvd(x_fvd, x_fvd_AR, device, only_final=True)['value'])\n",
    "        dcvc_eval['fvd'].append(calculate_fvd(x_fvd, 2 * x_fvd_dcvc - 1., device, only_final=True)['value'])\n",
    "\n",
    "\n",
    "def compute_stats(eval_dict):\n",
    "    stats = {}\n",
    "    for key, values in eval_dict.items():\n",
    "        values = np.array(values)\n",
    "        mean_val = np.mean(values)\n",
    "        std_val = np.std(values)\n",
    "        max_val = np.max(values)\n",
    "        min_val = np.min(values)\n",
    "        stats[key] = (mean_val, std_val, max_val, min_val)  # Normalize by length\n",
    "    return stats\n",
    "\n",
    "# Compute statistics for each evaluation method\n",
    "mse_stats = compute_stats(mse_eval)\n",
    "jd_stats = compute_stats(jd_eval)\n",
    "fmd_stats = compute_stats(fmd_eval)\n",
    "ar_stats = compute_stats(ar_eval)\n",
    "dcvc_stats = compute_stats(dcvc_eval)\n",
    "\n",
    "# Print formatted results\n",
    "print('FVD:    ' + \n",
    "      f'MSE {mse_stats[\"fvd\"][0]:.4f} | ' + \n",
    "      f'JD {jd_stats[\"fvd\"][0]:.4f} | ' +\n",
    "      f'FMD {fmd_stats[\"fvd\"][0]:.4f} | ' +\n",
    "      f'AR {ar_stats[\"fvd\"][0]:.4f} | ' +\n",
    "      f'DCVC {dcvc_stats[\"fvd\"][0]:.4f}')\n",
    "\n",
    "# print('Second frame MS-SSIM: ' + # f'MSE {mse_stats[\"2_ssim\"][0]:.4f}±{mse_stats[\"2_ssim\"][1]:.4f} | ' +\n",
    "#       f'JD {jd_stats[\"2_ssim\"][0]:.4f} | ' +\n",
    "#       f'FMD {fmd_stats[\"2_ssim\"][0]:.4f} | ' +\n",
    "#       f'AR {ar_stats[\"2_ssim\"][0]:.4f} | ' +\n",
    "#       f'DCVC {dcvc_stats[\"2_ssim\"][0]:.4f}')\n",
    "\n",
    "# print('Third frame PSNR:     ' + # f'MSE {mse_stats[\"3_psnr\"][0]:.4f}±{mse_stats[\"3_psnr\"][1]:.4f} | ' +\n",
    "#       f'JD {jd_stats[\"3_psnr\"][0]:.4f} | ' +\n",
    "#       f'FMD {fmd_stats[\"3_psnr\"][0]:.4f} | ' +\n",
    "#       f'AR {ar_stats[\"3_psnr\"][0]:.4f} | ' +\n",
    "#       f'DCVC {dcvc_stats[\"3_psnr\"][0]:.4f}')\n",
    "\n",
    "# print('Third frame MS-SSIM:  ' + # f'MSE {mse_stats[\"3_ssim\"][0]:.4f}±{mse_stats[\"3_ssim\"][1]:.4f} | ' +\n",
    "#       f'JD {jd_stats[\"3_ssim\"][0]:.4f} | ' +\n",
    "#       f'FMD {fmd_stats[\"3_ssim\"][0]:.4f} | ' +\n",
    "#       f'AR {ar_stats[\"3_ssim\"][0]:.4f} | ' +\n",
    "#       f'DCVC {dcvc_stats[\"3_ssim\"][0]:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "former-aircraft",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (3.6)",
   "language": "python",
   "name": "video_perc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
